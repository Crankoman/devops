# Домашнее задание к занятию 17 «Инцидент-менеджмент»

## Задание

Составьте постмортем на основе реального сбоя системы GitHub в 2018 году.

Информацию о сбое можно изучить по ссылкам ниже:

* [краткое описание на русском языке](https://habr.com/ru/post/427301/);
* [развёрнутое описание на английском языке](https://github.blog/2018-10-30-oct21-post-incident-analysis/).

<-- Ответ

## Отчет об инцеденте 21-22.10.18 GitHub

### краткое описание инцидента

Несколько сервисов на GitHub были затронуты сетевым сбоем и последующим сбоем базы данных, что привело к появлению противоречивой информации на веб-сайте.

### предшествующие события

Плановые работы по техническому обслуживанию для замены вышедшего из строя оптического оборудования привели к потере связи между сетевым центром на Восточном побережье США и основным ЦОД на Восточном побережье США. Связь между ними была восстановлена за 43 секунды, но это кратковременное отключение вызвало цепочку событий, которые привели к 24 часам и 11 минутам сбоя в работе сервиса.

### причина инцидента

Нарушение согласованности в Mysql-кластере, из-за разделения сети на две независимые части

### воздействие

Невозможность автоматического поддержания консистентности данных в кластере. Деградация кластера

### обнаружение

Внутренние системы мониторинга начали генерировать предупреждения, указывающие на то, что в системах наблюдались многочисленные сбои

### реакция

Команда реагирования решила вручную заблокировать  внутренний инструмент развертывания, чтобы предотвратить внесение каких-либо дополнительных изменений. Команда реагирования присвоила сайту желтый статус. Это действие автоматически перевело ситуацию в активный инцидент и отправило предупреждение координатору инцидента. Присоединился координатор инцидента и через две минуты изменил решение на красный статус

### восстановление

Приняли решение частично ухудшить удобство использования сайта, приостановив доставку webhook и сборку GitHub Pages вместо того, чтобы подвергать опасности данные, которые уже получили от пользователей.
Записали двоичные журналы MySQL, содержащие записи, сделанные основном сайте, которые не были реплицированы на сайт на Западном побережье из каждого затронутого кластера. 
### таймлайн

- 21.10.18 22:52 UTC: запланированная замена неисправного сетевого модуля привела к 43 секундной потере сетевой связности между двумя датацентрами, в которых находились члены MySQL кластеров. Кластера были настроены таким образом, что на запись работали основные ноды в датацентре US East Coast, а остальные ноды, в том числе расположенные в датацентре US West Coast, служили только для чтения. В результате потери связности между датацентрами произошел штатный фейловер: кластер выбрал нового лидера, была перестроена топология репликации, и запись данных стала осуществляться на ноды, расположенные в датацентре US West Coast. 
- 21.10.18 22:54 UTC: появились многочисленные алерты о сбоях в работе. Инженеры первой линии поддержки выявили, что это было вызвано изменившейся топологией кластеров MySQL.
- 21.10.18 23:07 UTC: кластер был переведен в ручной режим управления топологией. Это повысило приоритет инцидента, поэтому к работам подключились владельцы сервиса (DBA).
- 21.10.18 23:13 UTC: на этом этапе было обнаружено, что ни один из датацентров не содержит полной реплики данных: новые данные пишутся в датацентр US West Coast уже более получаса, но при этом в датацентре US East Coast содержится некоторое количество записей, которые не успели отреплицироваться из-за нарушения сетевой связности. Дополнительной проблемой являлось то, что из-за изменения топологии базы данных и приложения, их использующие, находятся в разных датацентрах, поэтому возникла дополнительная задержка записи, приводящая к многочисленным ошибкам у пользователей.
- 21.10.18 23:19 UTC: чтобы обеспечить целостность данных, было принято решение об отключении некоторых служб: pages jobs, webhooks, pushes.
- 22.10.18 00:05 UTC: разработан пошаговый план восстановления данных из бэкапа, восстановления топологии кластера и запуска остановленных сервисов.
- 22.10.18 00:41 UTC: начато восстановление из бэкапа. Поскольку данные бэкапа хранились в отдельной локации, то их передача, проверка целостности и распаковка заняли значительное количество времени.
- 22.10.18 06:51 UTC: включили репликацию свежих данных после завершения восстановления из бэкапа. Значительный объем данных репликации между датацентрами увеличил время отклика на пользовательские запросы. Несогласованность реплик приводила к тому, что пользователи могли получать устаревшие данные.
- 22.10.18 11:12 UTC: частично восстановлена правильная топология кластеров, что в общем повысило скорость пользовательских запросов, но так как репликация отдельных кластеров еще не была завершена, то пользователи всё еще получали некорректные ответы. Дополнительной проблемой служило то, что с наступлением дня повысилась пользовательская нагрузка, снизившая скорость восстановления данных.
- 22.10.18 13:15 UTC: пользовательская нагрузка достигла пиковых значений. В совокупности с задачами восстановления данных сервера перестали справляться с нагрузкой, поэтому были в кластера были добавлены новые ноды.
- 22.10.18 16:24 UTC: полностью восстановлена топология кластеров, начата работа по включению notification сервисов, отключенных ранее.
- 22.10.18 16:45 UTC: балансировка нагрузки и работа в деградируемом состоянии. увеличение TTL обработки webhooks 
- 22.10.18 23:03 UTC: надлежащая работа всех систем подтверждена

### последующие действия

- Измените конфигурацию Orchestrator, чтобы предотвратить распространение первичных баз данных через региональные границы.
- Ускорили переход на новый механизм отчетности о состоянии, который предоставит более богатый форум для обсуждения активных инцидентов более четким языком.
- Запустили общекорпоративную инженерную инициативу по поддержке обслуживания трафика GitHub из нескольких центров обработки данных в активном дизайне. Цель этого проекта - поддерживать резервирование N + 1 на уровне объекта. Цель этой работы - допустить полный отказ одного центра обработки данных без влияния пользователя.